
----------------------------------------
docker pull tensorflow/tensorflow
----------------------------------------

Start CPU only container
$ docker run -it -p 8888:8888 tensorflow/tensorflow

Start GPU (CUDA) container
Install nvidia-docker and run
$ nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu

----------------------------------------
docker pull xblaster/tensorflow-jupyter
----------------------------------------

With port forwarding:
docker run -d -p 8888:8888 xblaster/tensorflow-jupyter

For persistent storage:
docker run -d -p 8888:8888 -v /notebook:/notebook xblaster/tensorflow-jupyter

Just browse localhost:8888 and write code for tensorflow!

----------------------------------------
docker pull hytssk/tensorflow
----------------------------------------

For Linux
# Docker
$ docker run -d --name tensorflow -p 8888:8888 -p 6006:6006 -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix hytssk/tensorflow

# NVIDIA Docker
$ nvidia-docker run -d --name tensorflow -p 8888:8888 -p 6006:6006 -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix hytssk/tensorflow:gpu

For Windows(utilize MobaXTerm)

In boot2docker,
# Docker
$ docker run -d --name tensorflow -p 8888:8888 -p 6006:6006 -p 10022:22 hytssk/tensorflow


----------------------------------------
docker pull jupyter/tensorflow-notebook
----------------------------------------

run the following:

docker run -d -p 8888:8888 jupyter/tensorflow-notebook start-notebook.sh --NotebookApp.password='sha1:74ba40f8a388:c913541b7ee99d15d5ed31d4226bf7838f83a50e'

For example, to set the base URL of the notebook server, run the following:
docker run -d -p 8888:8888 jupyter/tensorflow-notebook start-notebook.sh --NotebookApp.base_url=/some/path

For example, to disable all authentication mechanisms (not a recommended practice):
docker run -d -p 8888:8888 jupyter/tensorflow-notebook start-notebook.sh --NotebookApp.token=''

You can sidestep the start-notebook.sh script and run your own command

SSL Certificates

You may mount SSL key and certificate files into a container and configure Jupyter Notebook to use them to accept HTTPS connections. For example, to mount a host folder containing a notebook.key and notebook.crt:

docker run -d -p 8888:8888 \
    -v /some/host/folder:/etc/ssl/notebook \
    jupyter/tensorflow-notebook start-notebook.sh \
    --NotebookApp.keyfile=/etc/ssl/notebook/notebook.key
    --NotebookApp.certfile=/etc/ssl/notebook/notebook.crt

Alternatively, you may mount a single PEM file containing both the key and certificate. For example:

docker run -d -p 8888:8888 \
    -v /some/host/folder/notebook.pem:/etc/ssl/notebook.pem \
    jupyter/tensorflow-notebook start-notebook.sh \
    --NotebookApp.certfile=/etc/ssl/notebook.pem


----------------------------------------
docker pull satoshun/tensorflow-notebook
----------------------------------------

How to use it?

$ docker pull satoshun/tensorflow-notebook
$ docker run --rm -it -p 8888:8888 -v "$HOME/notebooks:/notebooks" satoshun/tensorflow-notebook
$HOME/notebooks is your notebook direcotry of local.


----------------------------------------
docker pull erroneousboat/tensorflow-python3-jupyter
----------------------------------------

Tensorflow Python3 Jupyter

Docker container with python 3 version of tensorflow accompanied by jupyter
Usage

$ docker run -p 8888:8888 \
    -v [path-to-notebooks]:/notebooks \
    -it erroneousboat/tensorflow-python3-jupyter





----------------------------------------
docker pull udacity/carnd-tensorflow-lab
----------------------------------------

Clone the Repository
Run the command below to clone the Lab Repository:
$ git clone https://github.com/udacity/CarND-TensorFlow-Lab.git

Run the Notebook using Docker
Run the following command from the same directory as the command above.
$ docker run -it -p 8888:8888 -v `pwd`:/notebooks udacity/carnd-tensorflow-lab

View The Notebook

OS X and Linux Instructions
Install Anaconda

This lab requires Anaconda and Python 3.4 or higher. If you don't meet all of these requirements, install the appropriate package(s).
Run the Anaconda Environment

Run these commands in your terminal to install all the requirements:

$ git clone https://github.com/udacity/CarND-TensorFlow-Lab.git
$ conda env create -f CarND-TensorFlow-Lab/environment.yml
$ conda install --name CarND-TensorFlow-Lab -c conda-forge tensorflow

Run the Notebook

Run the following commands from the same directory as the commands above.

$ source activate CarND-TensorFlow-Lab
$ jupyter notebook


----------------------------------------
docker pull ornew/tensorflow-android
----------------------------------------

Docker: https://www.docker.com/

$ docker pull ornew/tensorflow-android
$ docker run -it ornew/tensorflow-android
# bash install
# . ~/.bashrc

Official Demo App Build

You execute bash demo/build on shell in docker:

# bash demo/build

You get .apk

You should mount host volume when new container ran. Use docker run -v <MOUNT_HOST_PATH>:<MOUNT_CONT_PATH> option.

Example:

$ docker pull ornew/tensorflow-android
$ docker run -it -v /tmp/tf/apk/:/usr/local/tf/ ornew/tensorflow-android
# bash install
# . ~/.bashrc
# bash demo/build
# cp ~/ornew/tensorflow-android/demo/tensorflow/bazel-bin/tensorflow/examples/android/tensorflow_demo.apk /usr/local/tf/
# exit
$ cd /tmp/tf/apk/
$ ls
tensorflow_demo.apk
$ adb install -r -g ./tensorflow_demo.apk

Performance improvement

    Use --net=host
    Use -v $(pwd)/bin:/root

----------------------------------------
docker pull nethsix/ruby-tensorflow-ubuntu
----------------------------------------

Tensorflow in Ruby using 'tensorflow' gem from https://github.com/somaticio/tensorflow.rb

* docker run --rm -it nethsix/ruby-tensorflow-ubuntu:0.0.1 /bin/bash

Location:

    /repos/tensorflow.rb/
* cd /repos/tensorflow.rb/ && bundle exec rspec

Image Classification Tutorial:

    cd /repos/tensorflow.rb/image
    cat README

----------------------------------------
docker pull cbiffle/docker-tensorflow
----------------------------------------

To build:

docker build github.com/cbiffle/docker-tensorflow:avx2-cuda

To run (from Docker Hub):

nvidia-docker run -it -p 8888:8888 cbiffle/docker-tensorflow:avx2-cuda

Or using an external notebook directory (which I recommend, as it makes the
container ephemeral):

nvidia-docker run -it -p 8888:8888 -v /path/to/project:/notebooks \
    cbiffle/docker-tensorflow:avx2-cuda


----------------------------------------
docker pull lukovkin/dockerfile-cuda-tensorflow-keras-jupyter
----------------------------------------

With port forwarding:
docker run -d -p 8888:8888 lukovkin/dockerfile-cuda-keras-jupyter

For persistent storage:
docker run -d -p 8888:8888 -v /notebook:/notebook lukovkin/dockerfile-cuda-keras-jupyter

Just browse localhost:8888 and write code with Keras!


----------------------------------------
docker pull frostbitelabs/tensorflow-base
----------------------------------------

Build

Builds the Docker image frostbitelabs/tensorflow-base,
docker build -t frostbitelabs/tensorflow-base .

or without cache,
docker build -t frostbitelabs/tensorflow-base --no-cache .

Push
docker login
docker push frostbitelabs/tensorflow-base

Run Python notebook from container
docker run -it --rm -p 8888:8888 frostbitelabs/tensorflow-base jupyter notebook

Run VNC from container
docker run -it --rm -p 5900:5900 frostbitelabs/tensorflow-base x11vnc -xdummy




----------------------------------------
docker pull adolphlwq/docker-tensorflow
----------------------------------------

    start tensorflow container
    docker run -p 8888:8888 -p 6006:6006 adolphlwq/docker-tensorflow:v1.0.0

    browser localhost:8888 or your_host_ip:8888 to see jupyter online editor

Custom: Docker Options

    set password to Jupyter Notebook
    docker run -p 8888:8888 -p 6006:6006 -e PASSWORD="YOURPASS" adolphlwq/docker-tensorflow:v1.0.0

    gives the container user:jovyan passwordless sudo capability
    docker run -p 8888:8888 -p 6006:6006 -e GRANT_SUDO=yes adolphlwq/docker-tensorflow:v1.0.0

    mount volume mapping from host to container
    docker run -p 8888:8888 -p 6006:6006 -v pathto/host/folder:/home/jovyan/work adolphlwq/docker-tensorflow:v1.0.0



----------------------------------------
docker pull chiefware/tensorflow-jupyter
----------------------------------------

first start as root and create user
docker run -d -p 8888:8888 chiefware/tensorflow-jupyter


Just browse localhost:8888 and create user in shell and save your settings

docker ps
docker commit <container id> chiefware/tensorflow-jupyter
docker kill <container id>
now start as user

With port forwarding:
docker run -u <created user>  -d -p 8888:8888 -p 6006:6006 chiefware/tensorflow-jupyter

Just browse localhost:8888 and write code for tensorflow!




----------------------------------------
docker pull erroneousboat/tensorflow-python3
----------------------------------------

Usage
$ docker run -it erroneousboat/tensorflow-python3 python



----------------------------------------
docker pull atong01/imagenet-tensorflow
----------------------------------------

Use the following command for the default image:
docker run atong01/imagenet-tensorflow python classify_image.py

or
sh run.sh

Or, to use your own image file:
docker run -v $PWD:/root/tmp:ro atong01/imagenet-tensorflow python classify_image.py --image_file tmp/$(IMAGE)

or
sh run.sh $(IMAGE)

----------------------------------------
docker pull cannin/jupyter-keras-tensorflow-tools
----------------------------------------

Build

docker build -t cannin/jupyter-keras-tensorflow-tools:tf-1.0.1 .
docker build -t cannin/jupyter-keras-tensorflow-tools:tf-1.0.1-gpu -f Dockerfile.gpu .

docker build -t cannin/jupyter-keras-tensorflow-tools-sshd:tf-1.0.1-py3 -f Dockerfile_ssh .

Run

docker rm -f keras; docker run --name keras -v DIR:/notebooks -p 8888:8888 -t cannin/jupyter-keras-tensorflow-tools:tf-1.0.1

docker exec -i -t keras bash

SSH

docker rm -f sshd; docker run -d --name sshd -p 23:22 -p 8888:8888 -v $(pwd):/notebooks -w /notebooks -t cannin/jupyter-keras-tensorflow-tools-sshd:tf-0.12.1-py3
docker rm -f sshd; docker run --name sshd -p 23:22 -p 8888:8888 -v $(pwd):/notebooks -w /notebooks -it cannin/jupyter-keras-tensorflow-tools-sshd:tf-0.12.1-py3 bash
docker exec -it sshd bash
ssh -p 23 root@localhost




----------------------------------------
docker pull 6thbridge/tensorflow-keras-docker
----------------------------------------

Building the image:

docker build -f Dockerfile -t keras-1 .

Running:

docker run -it keras-1

This will drop you to a bash shell in /root. From here you can launch any of
the Keras examples in ./keras/examples.
Example imdb_cnn.py:

cd ./keras/examples
python imdb_cnn.py

----------------------------------------
docker pull deepgnosis/tensorflow-conda-py3.5
----------------------------------------

Running the container
Run non-GPU container using

$ docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow

For GPU support install Nvidia drivers (ideally latest) and
nvidia-docker. Run using

$ nvidia-docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow:latest-gpu

Note: If you would have a problem running nvidia-docker you may try the old way
we have used. But it is not recomended. If you find a bug in nvidia-docker report
it there please and try using the nvidia-docker as described above.

$ export CUDA_SO=$(\ls /usr/lib/x86_64-linux-gnu/libcuda.* | xargs -I{} echo '-v {}:{}')
$ export DEVICES=$(\ls /dev/nvidia* | xargs -I{} echo '--device {}:{}')
$ docker run -it -p 8888:8888 $CUDA_SO $DEVICES gcr.io/tensorflow/tensorflow:latest-gpu

Rebuilding the containers

Just pick the dockerfile corresponding to the container you want to build, and run
$ docker build --pull -t $USER/tensorflow-suffix -f Dockerfile.suffix .

----------------------------------------
docker pull toshihikoyanase/tensorflow-rnn-work
----------------------------------------

docker-tensorflow-rnn-work

Working environment to develop rnn of tensorflow.
It runs build commands of rnn in advance.
Run

docker run -it toshihikoyanase/tensorflow-rnn-work /bin/bash

----------------------------------------
docker pull renarl/tensorflow-openai-gym
----------------------------------------

docker run -it -p 8888:8888 -p 6006:6006 renarl/tensorflow-openai-gym
Open http://localhost:8888/ in a browser.

----------------------------------------
docker pull ermaker/keras-jupyter
----------------------------------------


With port forwarding:
docker run -d -p 8888:8888 ermaker/keras-jupyter

With Tensorflow: (See this for more information)
docker run -d -p 8888:8888 -e KERAS_BACKEND=tensorflow ermaker/keras-jupyter

For persistent storage:
docker run -d -p 8888:8888 -v /notebook:/notebook ermaker/keras-jupyter

Just browse localhost:8888 and write code with Keras!

----------------------------------------
docker pull dash00/tensorflow-python3-jupyter
----------------------------------------

On Linux

    Open a terminal
    Install Docker
     $ sudo apt-get install docker.io

    Download the machine learning environment
     $ docker pull dash00/tensorflow-python3-jupyter

Disable token authentification

Even if it is not recommended for security reasons, the token authentification can be disabled. To remove it, you will have to explicitely call the run_jupyter.sh script with the option --NotebookApp.token set to empty:
 $ docker run -it -p 8888:8888 dash00/tensorflow-python3-jupyter /run_jupyter.sh --allow-root --NotebookApp.token=''

Use a persistent folder

If you want to work in persistent folder (independent of the container, which will not be removed at the end of the container execution) use the -v option as follow:
 $ docker run -it -p 8888:8888 -v /$(pwd)/notebooks:/notebooks dash00/tensorflow-python3-jupyter

You can change /$(pwd)/notebooks by any path on the local system. If the folder does not exist, it will be created. This option maps the given local folder with the folder of the notebooks on Jupyter. This folder should contain all your notebooks indeed.
Now, let's use Jupyter Notebook and Tensorboard in the same time

a. Create a container 'notebooks' to run Jupyter Notebook (port 8888)
 $ docker run  --name notebooks -d -v /$(pwd)/notebooks:/notebooks -v /$(pwd)/logs:/logs -p 8888:8888 dash00/tensorflow-python3-jupyter /run_jupyter.sh --allow-root --NotebookApp.token=''

The option -d detaches the container, i.e. it makes it run in background. Jupyter is still available on http://<CONTAINER_IP>:8888/.

 b. Create a container 'board' to run Tensorboard (port 6006):
 $ docker run  --name board -d -v /$(pwd)/logs:/logs -p 6006:6006 dash00/tensorflow-python3-jupyter tensorboard --logdir /logs

Tensorboard will be available on http://<CONTAINER_IP>:6006/.


How to build my own docker image ?

Get the files of the GitHub repository tensorflow-python3-jupyter by downloading the ZIP file or by cloning the repository:

git clone https://github.com/dash00/tensorflow-python3-jupyter.git

Open a terminal and set the current working directory to the newly created folder with the project files.

cd tensorflow-python3-jupyter
docker build .


----------------------------------------
docker pull lopezco/udacity-tensorflow
----------------------------------------

Running the Docker container from the Dockerhub image

docker run -p 8888:8888 -p 6006:6006 --name udacity-tensorflow -it lopezco/udacity-tensorflow

Note that if you ever exit the container, you can return to it using:

docker start -ai udacity-tensorflow

Environment Variables

You can set some environment variables while running the container using the --env parameter as follows:

--env JUPYTER_PASSWORD="my_password" --env JUPYTER_PORT=8888 --env TENSORBOARD_LOGDIR="./logs" --env TENSORBOARD_PORT=6006

Accessing the Notebooks

    Jupyter: http://localhost:8888
    Tensorboard: http://localhost:6006


Running the Docker container with docker-compose

Install docker-compose following this guide.
Then, download this docker-compose file.

docker-compose up [-d]

This command will pull the image from docker-hub and automatically run the container. The parameter -d is optional (run in detached mode)

Note that if you ever stop the container, you can restart it using:

docker-compose up [-d]

Environment Variables

You can set some environment variables in the docker-compose file to control the container:

environment:
  JUPYTER_PASSWORD: "my_password"
  JUPYTER_PORT: 8899
  TENSORBOARD_LOGDIR: ./logs
  TENSORBOARD_PORT: 8008

Accessing the Notebooks

    Jupyter: http://localhost:8899
    Tensorboard: http://localhost:8008


----------------------------------------
docker pull jessewei/tensorflow-watson
----------------------------------------

Running the container
Run non-GPU container using
$ docker run -it -p 8888:8888 jessewei/tensorflow-watson


----------------------------------------
docker pull tensorflow/magenta
----------------------------------------
See installation instructions on our GitHub page: https://github.com/tensorflow/magenta

----------------------------------------
docker pull claytantor/tensorflow-lstm-regression
----------------------------------------

Docker container

$ docker run -d -p 8888:8888 claytantor/tensorflow-lstm-regression:latest

Version Info

Building The Project
Create a Virtual Environment

It is reccomended that you create a virtualenv for the setup since this example is highly dependant on the versions set in the requirements file.

$ virtualenv ~/python/ltsm
$ source ~/python/ltsm/bin/activate
(ltsm) $

Install Requirements

This example depends on tensorflow-0.10.0rc0 to work. You will first need to install the requirements. You will need the appropriate version of tensorflow for your platform, this example is for mac. For more details goto TAG tensorflow-0.10.0rc0 Setup

(ltsm) $ wget https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0rc0-py2-none-any.whl
(ltsm) $ pip install -U ./tensorflow-0.10.0rc0-py2-none-any.whl
(ltsm) $ pip install -r ./requirements.tx

Running on Jupyter

Three Jupyter notebooks are provided as examples on how to use lstm for predicting shapes. They will be available when you start up Jupyter in the project dir.

(ltsm) $ jupyter notebook


----------------------------------------
docker pull stealthinu/alpine-py3-tensorflow-jupyter-datasets
----------------------------------------

How to use this image

Please run the following

docker run -itd -p 8888:8888 -e PASSWORD=foobar stealthinu/alpine-py3-tensorflow-jupyter-datasets

and access to http://{docker host}:8888/. It opens Jupyter's login panel so
please enter the password which you specified as PASSWORD environment value

This repository also provides Docker Compose example so you can boot a
container of this image by running

docker-compose up

in docker_compose_example directory.


----------------------------------------
docker pull kmubigdata/ubuntu-tensorflow
----------------------------------------

sudo nvidia-docker run -it --name <container_name> -v /usr/local/cuda-8.0/:/usr/local/cuda-8.0/ kmubigdata/ubuntu-tensorflow /bin/bash

----------------------------------------
docker pull imcomking/bi_deeplearning
----------------------------------------


General Usage
# create container with gpu device:

docker run -it --device /dev/nvidiactl --device /dev/nvidia-uvm --device /dev/nvidia0 --dns=8.8.8.8 -p 22 -p 6006 -p 8888 imcomking/bi_deeplearning

if you get an error message [ "/dev/nvidia-uvm": lstat /dev/nvidia-uvm: no such file or directory ], firstly try to run [ nvidia-smi ] on your host machine. Because "/dev/nvidia-uvm" is created sometimes when nvidia-smi is firstly called. Otherwise try these commands on your host machine.

    sudo apt-get install nvidia-modprobe
    sudo modprobe nvidia-uvm
    sudo mknod -m 666 /dev/nvidia-uvm c 250 0



----------------------------------------
docker pull evau23/tensorflow-https-server
----------------------------------------

to run this container

docker run -d -p 8888:8888 -e "PASSWORD=your_password" evau23/tensorflow-https-server:latest  bash /run_jupyter.sh



----------------------------------------
docker pull iimuz/jupyter-tensorflow-notebook
----------------------------------------
The following command starts a container with the Notebook server listening for HTTP connections on port 8888 with a randomly generated authentication token configured.

docker run -it --rm -p 8888:8888 jupyter/tensorflow-notebook

Notebook Options

The Docker container executes a start-notebook.sh script script by default. The start-notebook.sh script handles the NB_UID, NB_GID and GRANT_SUDO features documented in the next section, and then executes the jupyter notebook.

You can pass Jupyter command line options through the start-notebook.sh script when launching the container. For example, to secure the Notebook server with a custom password hashed using IPython.lib.passwd() instead of the default token, run the following:

docker run -d -p 8888:8888 jupyter/tensorflow-notebook start-notebook.sh --NotebookApp.password='sha1:74ba40f8a388:c913541b7ee99d15d5ed31d4226bf7838f83a50e'

For example, to set the base URL of the notebook server, run the following:

docker run -d -p 8888:8888 jupyter/tensorflow-notebook start-notebook.sh --NotebookApp.base_url=/some/path

For example, to disable all authentication mechanisms (not a recommended practice):

docker run -d -p 8888:8888 jupyter/tensorflow-notebook start-notebook.sh --NotebookApp.token=''

You can sidestep the start-notebook.sh script and run your own commands in the container. See the Alternative Commands section later in this document for more information.

SSL Certificates

You may mount SSL key and certificate files into a container and configure Jupyter Notebook to use them to accept HTTPS connections. For example, to mount a host folder containing a notebook.key and notebook.crt:

docker run -d -p 8888:8888 \
    -v /some/host/folder:/etc/ssl/notebook \
    jupyter/tensorflow-notebook start-notebook.sh \
    --NotebookApp.keyfile=/etc/ssl/notebook/notebook.key
    --NotebookApp.certfile=/etc/ssl/notebook/notebook.crt

Alternatively, you may mount a single PEM file containing both the key and certificate. For example:

docker run -d -p 8888:8888 \
    -v /some/host/folder/notebook.pem:/etc/ssl/notebook.pem \
    jupyter/tensorflow-notebook start-notebook.sh \
    --NotebookApp.certfile=/etc/ssl/notebook.pem

----------------------------------------
docker pull volnet/tensorflow-jupyter
----------------------------------------

Get started

The next scripts can help you to run jupyter notebook and tensorboard at one time:

git clone https://github.com/volnet/tensorflow-server.git
cd tensorflow-server
./tf_start_all.sh

You can stop all of them by :

./tf_stop_all.sh

How to run

The shell script can help you to run docker:

docker run -d -p 8888:8888 -v $(pwd)/notebooks:/notebooks -v $(pwd)/logs:/logs --name my-tf-notebook volnet/tensorflow-jupyter





----------------------------------------
docker pull andrewdacenko/tensorflow-nlp
----------------------------------------


Start CPU only container

$ docker run -it -p 8888:8888 andrewdacenko/tensorflow-nlp:1.0.0
Go to your browser on http://localhost:8888/


----------------------------------------
docker pull vegaxju/tensorflow-hdfs
----------------------------------------

docker build -t <image_name>:v1 -f Dockerfile .
# Use gcloud docker push instead if on Google Container Registry.
docker push <image_name>:v1

If you make any updates to the code, increment the version and rerun the above
commands with the new version.


----------------------------------------
docker pull clarkchan/rpi-tensorflow-imagenet
----------------------------------------


step 1:prepare your image file in a working path. for example: put a.jpg in $PWD ( $PWD means current directory)
step 2:pull the image
docker pull clarkchan/tensorflow_imagenet
step 3:run it
docker run -it -v $PWD:/images clarkchan/rpi-tensorflow-imagenet python /root/classify_image.py --image_file=/images/a.jpg

it will give a result like :
dogsled, dog sled, dog sleigh (score = 0.39597)
ski (score = 0.21072)
alp (score = 0.14548)
snowmobile (score = 0.01724)
bobsled, bobsleigh, bob (score = 0.00864)


----------------------------------------
docker pull rankwinner/tensorflow_python3
----------------------------------------

Create docker container using
    docker run -p 8888:8888 -p 6006:6006 -it --name [name] -v /home/shared-dir/notebooks:/notebooks rankwinner/tensorflow_python3:latest
    04a. shared-dir/notebooks should already be mounted, if not use
        mount /home/shared-dir/notebooks:/notebooks


----------------------------------------
docker pull ivanturianytsia/tensorflow-jupyter-docker
----------------------------------------

Usage:
docker run -p <host port>:8888 -v <host work directory>:/opt/notebooks/<container work directory> ivanturianytsia/tensorflow-jupyter:<version>

Example
docker run -p 8888:8888 -v $PWD:/opt/notebooks/notebook1 ivanturianytsia/tensorflow-jupyter:latest

----------------------------------------
docker pull 20041994/tensorflow-flask-apache
----------------------------------------

Step 1: docker pull 20041994/tensorflow-flask-apache

Step 3: docker run -it -p 8080:80 <image name>:<tag>

cd /flaskapp
in flaskapp.py is main route of my application, modify it with your service

Link : https://bitbucket.org/phong204/docker-flask-tensorflow

----------------------------------------
docker pull charles2014/tensorflow-ubuntu-ssh-pycharm
----------------------------------------
tensorflow + ubuntu + ssh + pycharm



----------------------------------------
docker pull devjulian/app-restaurant-udacity
----------------------------------------
#Testing

docker-compose -f test.yml run db_testing sqlite3 testing.db

----------------------------------------
docker pull skuenzli/udacity-networking
----------------------------------------


Running

To use this image with the exercises, run:

docker run --rm -it --name udacity_networking skuenzli/udacity-networking:2017-01-01

Another Shell

In order to enter the container from another shell, use docker exec to start another bash process:

# the ps command inside the $() finds the id of the running udacity_networking container:
docker exec -it $(docker ps --quiet --filter name=udacity_networking) /bin/bash

Type exit to leave the container.


Resources

    Source Repository: https://github.com/skuenzli/docker-udacity-networking
    Course: https://www.udacity.com/course/networking-for-web-developers--ud256



----------------------------------------
docker pull xjiajiahao/tensorflow-udacity
----------------------------------------

Full Description
docker run -p 8888:8888 --name tensorflow-udacity -it xjiajiahao/tensorflow-udacity

----------------------------------------
docker pull rogercaminal/udacity-deeplearning
----------------------------------------

Assignments from Udacity Deep Learning course.

For more information, see https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity

----------------------------------------
docker pull zavolokas/tensorflow-udacity
----------------------------------------

Image with installed tools to accomplish Deep Learning course assignments on Udacity. This image includes Pandas installation.

Create and start a container for the first time as follows:
docker run --name tensorflow-udacity-pandas -it -p 8888:8888 zavolokas/tensorflow-udacity:pandas

Next times just start the created container:
docker start -ai tensorflow-udacity-pandas
Dockerfile

FROM gcr.io/tensorflow/udacity-assignments:1.0.0
RUN apt-get update -y && apt-get install -y python-pandas
EXPOSE 8888

----------------------------------------
docker pull kkcthans/tensorflow_udacity
----------------------------------------

Personal Image of Udacity Deep Learning Class from:
https://www.udacity.com/course/deep-learning--ud730


----------------------------------------
docker pull stevekc/docker-with-continuous-integration
----------------------------------------


Test app for bTreePress course on Continous Integration with Docker

This is a quick node.js appfor the purposes of demonstrating a basic CI/CD workflow with Docker Hub for How To Use Docker With Continous Integration To Build A DevOps Automated Workflow .
This Project Is Included In the Exercise Files
and @
GitHub:
https://github.com/bTreePress/Docker-With-Continous-Integration
Instructions

download or pull to your local computer
https://github.com/bTreePress/Docker-With-Continous-Integration.git

In the root of the project folder run

npm install

To Start the site run

node .

To View The Site go to:
http://localhost:8080



----------------------------------------
docker pull jamesmstone/weka
----------------------------------------

A dockerised version of Weka 3: Data Mining Software in Java http://www.cs.waikato.ac.nz/ml/weka/

Usage
GUI

docker run --rm \
    -v "$(pwd)" \
    -w "$(pwd)" \
    -e DISPLAY=$DISPLAY \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    jamesmstone/weka "$@"



----------------------------------------
docker pull jwenzel/xubuntu-pdi-weka
----------------------------------------


XFCE Desktop with WEKA 3.8 and PDI 6.1 Installed

Full Description
docker run -d -p 0.0.0.0:3389:3389 -v /home/jwenzel:/home/jwenzel -v /mapr:/mapr jwenzel/xubuntu-pdi-weka:v1.0 /sbin/my_init



----------------------------------------
docker pull zzz1990771/r-java-weka
----------------------------------------

Log in using:
username: rstudio
password: rstudio

To use this image, just run:
sudo docker run -d -p 8787:8787 --name rstudio -v /home/Dockerdata:/home/rstudio/data zzz1990771/r-java-weka


Notes: the shared data volume will be /home/Dockerdata in host mapped to /home/rstudio/data in container.


----------------------------------------
docker pull willprice/coms30301-jupyter-notebook
----------------------------------------
Usage

$ mkdir notebooks
$ docker run -p 8888:8888 -p "$PWD:notebooks:/home/jovyan/work" willprice/jupyter-r-weka

Finally point your brower to http://localhost:8888 to access the Jupyter
notebook.



Build

$ docker build -t willprice/coms30301-jupyter-notebook .


----------------------------------------
docker pull nshou/elasticsearch-kibana
----------------------------------------
docker run -d -p 9200:9200 -p 5601:5601 nshou/elasticsearch-kibana

Then you can connect to Elasticsearch by localhost:9200 and its Kibana front-end by localhost:5601.

----------------------------------------
docker pull bbania/elk-kibana
----------------------------------------
Kibana development container for Jelastic PaaS
Dockerfile at https://github.com/bubbl/elk-kibana/blob/master/Dockerfile


----------------------------------------
docker pull pschiffe/rsyslog-elasticsearch-kibana
----------------------------------------

To get this image, pull it from docker hub:

$ docker pull pschiffe/rsyslog-elasticsearch-kibana

Or, if you want to build this image yourself, clone the github repo and in directory with Dockerfile run:

$ docker build -t <username>/rsyslog-elasticsearch-kibana .

To run the image use:

$ docker run -d -p 514:514 -p 514:514/udp -p 5601:5601 -v /etc/localtime:/etc/localtime:ro pschiffe/rsyslog-elasticsearch-kibana

Rsyslog listens on standard port 514 (both TCP and UDP) and kibana on TCP port 5601. To forward log messages from your system, configure rsyslog according to this recipe with appropriate address of running container. To test the running container from the host system you can use:

$ logger -n localhost 'log message from host'

Kibana is available via regular web browser on http://localhost:5601 address from the host system. Please note, that it can take up to 10 seconds for container to be ready after start.

Elasticsearch is storing data in docker data volume /var/lib/elasticsearch.


----------------------------------------
docker pull stakater/kibana-with-consul-template
----------------------------------------

docker run stakater/kibana-with-consul-template:latest

This image is intended to be run together with Consul and Consul-Template

The daemon consul-template queries a Consul instance and updates any number of specified templates on the file system. As an added bonus, consul-template can optionally run arbitrary commands when the update process completes.

consul-template -consul-addr=$CONSUL_URL -template="/templates/kibana.ctmpl:/opt/kibana/config/kibana.yml:service kibana restart"

Consul Key-Value entries:

    The URL to the Elasticsearch instance to use for all your queries should be defined by the key /kibana/elasticsearchURL. It should be a complete url including the port as well. You may use consul dns entries too. Example: http://elasticsearch-9200:9200, where elasticsearch-9200 is a consul dns entry.

Note: If the key /kibana/elasticsearchURL does not exist, the property will be assigned default value of http://elasticsearch:9200.

    If you are running kibana behind a proxy, and want to mount it at a path specify that path by the key /kibana/basePath. The basePath can't end in a slash. Example: /kibana



----------------------------------------
docker pull kodamap/kibana-elasticsearch
----------------------------------------

docker
elasticsearch + kibana

    create data directories

sudo mkdir -p /var/data/elasticsearch; sudo chmod 777 /var/data/elasticsearch
sudo mkdir -p /var/data/kibana; sudo chmod 777 /var/data/kibana

    build and run

cd dockerfiles
sudo docker build -t localhost/elasticsearch:v1 elasticsearch/
sudo docker build -t localhost/kibana:v1 kibana/
sudo docker run -v /var/data/elasticsearch:/var/lib/elasticsearch -p 9200:9200 -itd --name elasticsearch localhost/elasticsearch:v1
sudo docker run -v /var/data/kibana:/var/lib/kibana -p 5601:5601 -itd --name kibana --link elasticsearch:EL localhost/kibana:v1

nginx (optional)

If you need to proxy access and authentication is required for kibana, you may build nginx (reverse proxy ).

    nginx use 5681/tcp port.
    basic auth id / pass : elastic/changeme

sudo docker build -t localhost/nginx:v1 dockerfiles/nginx/
sudo docker run -p 5681:5681 -itd --name nginx --link kibana:KIBANA localhost/nginx:v1

Reference

    elasticsearch

https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html

    plugin

https://www.elastic.co/guide/en/beats/filebeat/current/_tutorial.html

    kibana

https://www.elastic.co/guide/en/kibana/current/rpm.html

    filebeat

https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation.html

    packetbeat

https://www.elastic.co/guide/en/beats/packetbeat/current/packetbeat-installation.html

----------------------------------------
docker pull beh01der/logstash-es-kibana
----------------------------------------

This all-in-one logstash stack image includes:

    JDK 8
    Logstash 1.4.2
    Elasticsearch 1.4.4
    Kibana 4.0.1

To start container, simply run

docker run -d \
   --name logstash \
   -p 5601:5601 \
   beh01der/logstash-es-kibana

Kibana web interface will become available on port 5601.

To customise logstash configuration, simply map (override) logstash.conf file

docker run -d \
   --name logstash \
   -p 5601:5601 \
   -v path-to-your-logstash-conf:/opt/logstash/conf/logstash.conf \
   beh01der/logstash-es-kibana

For debugging you can run this image in interactive mode

docker run -i -t \
   --name logstash \
   -p 5601:5601 \
   -v path-to-your-logstash-conf:/opt/logstash/conf/logstash.conf \
   beh01der/logstash-es-kibana \
   /bin/bash

then inside the container run:
/opt/start.sh


----------------------------------------
docker pull hbussell/docker-fluentd-kibana
----------------------------------------

Running the image

First step is to pull the image:

docker pull hbussell/docker-fluentd-kibana

Run the image:

Note you may want to change the volume -v parameter to set a different data directory used by Elasticsearch.

sudo docker run -p 9200:9200 -p 9300:9300 -p 24224:24224 -p 80:80 -v /data:/data  --name docker_fluentd_kibana_inst -i -t hbussell/docker-fluentd-kibana:docker-fluentd-kibana

Running with systemd:

    copy to systemd.fluentd-kibana.service to /etc/systemd/system
    sudo systemctl enable /etc/systemd/system/systemd.fluentd-kibana.service
    sudo systemctl start systemd.fluentd-kibana.service

----------------------------------------
docker pull ibeauvais/elasticsearch-kibana
----------------------------------------

elasticsearch-kibana

docker image with elasticsearch 5.2.2 and kibana

docker run -p 9200:9200 -p 5601:5601 ibeauvais/elasticsearch-kibana



----------------------------------------
docker pull qxip/docker-elassandra-kibana
----------------------------------------

Docker Elassandra 2.4.2 + Kibana 4.6
Full Description
docker-elassandra-kibi

Docker Elassandra 2.4.2 + Kibana 4.6

docker run -ti --rm --name elassandra -p 9200:9222 -p 5601:5601 -P qxip/docker-elassandra-kibana

----------------------------------------
docker pull kaixhin/cuda-torch
----------------------------------------

cuda-torch

Ubuntu Core 14.04 + CUDA 7.0 + cuDNN v4 + Torch7 (including iTorch).
Requirements

    NVIDIA Docker - see requirements for more details.

Usage

Use NVIDIA Docker: nvidia-docker run -it kaixhin/cuda-torch.

For more information on CUDA on Docker, see the repo readme.

To use Jupyter/iTorch open up the appropriate port. For example, use nvidia-docker run -it -p 8888:8888 kaixhin/cuda-torch. Then run jupyter notebook --ip="0.0.0.0" --no-browser to open a notebook on localhost:8888.



----------------------------------------
docker pull kaixhin/torch
----------------------------------------

Ubuntu Core 14.04 + Torch7 (including iTorch).
Full Description


torch

Ubuntu Core 14.04 + Torch7 (including iTorch).
Usage

To use Jupyter/iTorch open up the appropriate port. For example, use docker run -it -p 8888:8888 kaixhin/torch. Then run jupyter notebook --ip="0.0.0.0" --no-browser to open a notebook on localhost:8888.



----------------------------------------
docker pull floydhub/torch
----------------------------------------

Torch

A basic image with Torch installed. Contains minimal additional packages.

The GPU versions of Torch timeout when building on Docker Hub. They have to be built locally and pushed to the Docker Registry.

To build Docker image locally (Note: takes a few hours):

    Python 2: docker build -t floydhub/torch:latest-gpu-py2 -f Dockerfile-py2.gpu .
    Python 3: docker build -t floydhub/torch:latest-gpu-py3 -f Dockerfile-py3.gpu .

To push image to Docker registry:

    Python 2: docker push floydhub/torch:latest-gpu-py2
    Python 3: docker push floydhub/torch:latest-gpu-py3

----------------------------------------
docker pull kaixhin/cuda-torch-deps
----------------------------------------

Graphical applications

Starting graphical (X11) applications is possible with the following commands:

docker run -it `# Running interactively, but can be replaced with -d for daemons` \
  -e DISPLAY `# Pass $DISPLAY` \
  -v=/tmp/.X11-unix:/tmp/.X11-unix `# Pass X11 socket` \
  --ipc=host `# Allows MIT-SHM` \
  <image>

On Mac OS X, use XQuartz and allow connections from network clients. Then the following can be used:

docker run -it \
  -e DISPLAY=`ifconfig en0 | grep inet | awk '$1=="inet" {print $2}'`:0 `# Use XQuartz network $DISPLAY` \
  --ipc=host \
  <image>


Daemonising containers

Most containers run as a foreground process. To daemonise (in Docker terminology, detach) such a container it is possible to use:

docker run -d <image> sh -c "while true; do sleep 1; done"

It is now possible to access the daemonised container, for example using bash:

docker exec -it <id> bash


Sibling containers

To start containers on the host from within a docker container, the container requires docker-engine installed, with the same API version as the Docker daemon on the host. The Docker socket also needs to be mounted inside the container:

-v /var/run/docker.sock:/var/run/docker.sock


----------------------------------------
docker pull nightseas/cuda-torch
----------------------------------------

Requirement

    NVIDIA Docker - see requirements for more details.

Test The Image

nvidia-docker run -it nightseas/cuda-torch bash
luajit -l torch -e 'torch.test()'
luajit -l nn -e 'nn.test()'

luajit -l cutorch -e 'cutorch.test()'
luajit -l cunn -e 'nn.testcuda()'


----------------------------------------
docker pull gforge/nnx-torch
----------------------------------------

Torch Docker image

Ubuntu 14.04 + Torch + CUDA + cuDNN


Build the image using the following command:

./update.sh && docker build -t gforge/nnx-torch nnx-torch

You will also need to install nvidia-docker, which we will use to start the container with GPU access. This can be found at NVIDIA/nvidia-docker.
Usage
iTorch notebook

NV_GPU=0 nvidia-docker run --rm -it --volume=/path/to/notebook:/root/notebook \
  --env=JUPYTER_PASSWORD=my_password --publish=8888:8888 gforge/nnx-torch

Replace /path/to/notebook with a directory on the host machine that you would like to store your work in.

Then, when running the container, pass the following option to mount the configuration file into the container:

--volume=/path/to/notebook.json:/root/.jupyter/nbconfig/notebook.json

You should now notice that your notebooks are configured accordingly.


----------------------------------------
docker pull kaixhin/cuda-torch-mega
----------------------------------------


Usage

Use NVIDIA Docker: nvidia-docker run -it kaixhin/cuda-torch-mega.

For more information on CUDA on Docker, see this readme.

To use Jupyter/iTorch open up the appropriate port. For example, use nvidia-docker run -it -p 8888:8888 kaixhin/cuda-torch-mega. Then run jupyter notebook --ip="0.0.0.0" --no-browser to open a notebook on localhost:8888.


----------------------------------------
docker pull abhishekkr/ml-torch
----------------------------------------



    Try out Torch, a Scientific Computation (Machine Learnin)g framework powered by luajit.

    Prepared using instructions from "http://torch.ch/docs/getting-started.html"

    Dockerfile used can be found at https://github.com/abhishekkr/tutorials_as_code/blob/master/misc_sources/machiine-learning/torch/Dockerfile

----------------------------------------
docker pull jacopofar/docker-torch-rnn
----------------------------------------

Torch LSTM/RNN based character level text generator, this version includes an HTTP interface

CPU Only

    Start bash in the container
        docker run --rm -ti jacopofar/torch-rnn:base bash

    Preprocess the sample data

     python scripts/preprocess.py \
     --input_txt data/tiny-shakespeare.txt \
     --output_h5 data/tiny-shakespeare.h5 \
     --output_json data/tiny-shakespeare.json

    Train

     th train.lua \
     -input_h5 data/tiny-shakespeare.h5 \
     -input_json data/tiny-shakespeare.json \
     -gpu -1

    Sample
        th sample.lua -checkpoint cv/checkpoint_10000.t7 -length 2000 -gpu -1

TODO GPU support not present yet
CUDA

    Install nvidia-docker
        https://github.com/NVIDIA/nvidia-docker
    Start bash in the container
        nvidia-docker run --rm -ti crisbal/torch-rnn:cuda7.5 bash

    Preprocess the sample data

     python scripts/preprocess.py \
     --input_txt data/tiny-shakespeare.txt \
     --output_h5 data/tiny-shakespeare.h5 \
     --output_json data/tiny-shakespeare.json

    Train

     th train.lua \
     -input_h5 data/tiny-shakespeare.h5 \
     -input_json data/tiny-shakespeare.json

    Sample
        th sample.lua -checkpoint cv/checkpoint_10000.t7 -length 2000



----------------------------------------
docker pull daboos/torch_rnn
----------------------------------------

Short Description
Torch image with Torch_rnn installed: https://github.com/jcjohnson/torch-rnn



----------------------------------------
docker pull gtarobotics/udacity-sdc
----------------------------------------

Tensorflow GPU, CUDA, CuDNN, Keras, Caffe, Torch, Jupyter Notebook, ROS Indigo and Autoware

Test nvidia-smi

sudo nvidia-docker run --rm nvidia/cuda nvidia-smi

Then run the commands:
Run gtarobotics/udacity-sdc image

For GPU mode run:
sudo xhost +
sudo nvidia-docker run --env="DISPLAY" --volume="$HOME/.Xauthority:/root/.Xauthority:rw" -env="QT_X11_NO_MITSHM=1" -v /dev/video0:/dev/video0 -v /tmp/.X11-unix:/tmp/.X11-unix:ro -it -p 8888:8888 -p 6006:6006 -v ~/sharefolder:/sharefolder:shared gtarobotics/udacity-sdc bash

For CPU mode run (no need to install nvidia-docker):
sudo xhost +
sudo docker run --env="DISPLAY" --volume="$HOME/.Xauthority:/root/.Xauthority:rw" -env="QT_X11_NO_MITSHM=1" -v /dev/video0:/dev/video0 -v /tmp/.X11-unix:/tmp/.X11-unix:ro -it -p 8888:8888 -p 6006:6006 -v ~/sharefolder:/sharefolder:shared gtarobotics/udacity-sdc bash

In sharefolder you can copy data sets and code or link folders like this (while in the host, not in the container):

cd ~/sharefolder/
mkdir sdc-data #this will be the root of SDC data
sudo mount --bind /path_to_root_folder_with_datasets/ sdc-data

o test a nice CNN in Tensorflow do (while you are in docker container):

cd /sharefolder
git clone https://github.com/SullyChen/Nvidia-Autopilot-TensorFlow
cd Nvidia-Autopilot-TensorFlow

Download the dataset and extract into Nvidia-Autopilot-TensorFlow folder (it will create driving_dataset folder):
https://drive.google.com/file/d/0B-KJCaaF7ellQUkzdkpsQkloenM/view?usp=sharing

python3 train.py

when done (could take a few hours depending on your GPU) do:

python3 run_dataset.py

and you'll be impressed :-)
To run Jupyter Notebook do:

cd
./run_jupyter.sh

Then load a Jupyter Notebook open this page in the browser:
http://your_docker_host_machine_ip:8888/
To run Autoware do:

cd
./run_autoware.sh


----------------------------------------
docker pull mbartoli/char-rnn
----------------------------------------

Short Description
Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch


docker-char-nn

Docker container for use with Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch
Usage

Run docker run -it mbartoli/char-rnn
Persistent checkpoint

    Create a folder cv to persist training data with mkdir cv.
    Mount the folder into the container and run it:
    docker run -v $(pwd)/cv:/home/char-rnn/cv -it mbartoli/char-rnn
    Train your char-rnn

Custom training data

    Create a folder containing some training data.
    mkdir -p data/my-training-data
    Run the container with the new training data and cv folder mounted
    docker run -v $(pwd)/cv:/home/char-rnn/cv -v $(pwd)/data/my-training-data:/home/char-rnn/data/my-training-data -it mbartoli/char-rnn
    Train your char-rnn

Training and sampling

See the documentation on how to train and sample your char-rnn.
More Information

Docker Hub: mbartoli/char-nn
https://github.com/karpathy/char-rnn
http://karpathy.github.io/2015/05/21/rnn-effectiveness/


----------------------------------------
docker pull dominiek/deep-base
----------------------------------------

Deep learning base image (Tensorflow, Caffe, MXNet, Torch, Openface, etc.)

Supported DL frameworks:

    Tensorflow
    Caffe
    MXNet
    Torch

Other ML frameworks included:

    Python / SciPy / Numpy
    Scikit-Learn
    Scikit-Image
    OpenFace

https://github.com/dominiek/deep-base

----------------------------------------
docker pull mhasa004/ubuntu14.04-cuda7.0-torchcaffe
----------------------------------------
ubuntu14.04-cuda7.0-torch7.0-caffe



----------------------------------------
docker pull bethgelab/jupyter-deeplearning-x
----------------------------------------
Docker Image with Jupyter for Deep Learning (Caffe, Torch, Theano, Lasagne, Keras).


----------------------------------------
docker pull vijaygabale/deep-learning-packages
----------------------------------------
Contains deep learning packages such as Torch, Caffe, TensorFlow


----------------------------------------
docker pull kkpoon/neural-style
----------------------------------------

install the neural style in torch7 by https://github.com/jcjohnson/neural-style

run with

$ docker run -it --rm -v /your-img-folder:/data kkpoon-neural-style
# th neural_style.la -gpu -1 -style_image style_image.jpg -content_image content_image.jpg -output_image /data/output_image.png


----------------------------------------
docker pull waterfire/all-in-one-deep-learning-docker
----------------------------------------

CPU Version

docker pull floydhub/dl-docker:cpu

Option 2: Build the Docker image locally

Alternatively, you can build the images locally. Also, since the GPU version is not available in Docker Hub at the moment, you'll have to follow this if you want to GPU version. Note that this will take an hour or two depending on your machine since it compiles a few libraries from scratch.

git clone https://github.com/saiprashanths/dl-docker.git
cd dl-docker

CPU Version

docker build -t floydhub/dl-docker:cpu -f Dockerfile.cpu .

GPU Version

docker build -t floydhub/dl-docker:gpu -f Dockerfile.gpu .

This will build a Docker image named dl-docker and tagged either cpu or gpu depending on the tag your specify. Also note that the appropriate Dockerfile.<architecture> has to be used.

Running the Docker image as a Container

Once we've built the image, we have all the frameworks we need installed in it. We can now spin up one or more containers using this image, and you should be ready to go deeper

CPU Version

docker run -it -p 8888:8888 -p 6006:6006 -v /sharedfolder:/root/sharedfolder floydhub/dl-docker:cpu bash

GPU Version

nvidia-docker run -it -p 8888:8888 -p 6006:6006 -v /sharedfolder:/root/sharedfolder floydhub/dl-docker:gpu bash

Note the use of nvidia-docker rather than just docker

Parameter 	Explanation
-it 	This creates an interactive terminal you can use to iteract with your container
-p 8888:8888 -p 6006:6006 	This exposes the ports inside the container so they can be accessed from the host. The format is -p <host-port>:<container-port>. The default iPython Notebook runs on port 8888 and Tensorboard on 6006
-v /sharedfolder:/root/sharedfolder/ 	This shares the folder /sharedfolder on your host machine to /root/sharedfolder/ inside your container. Any data written to this folder by the container will be persistent. You can modify this to anything of the format -v /local/shared/folder:/shared/folder/in/container/. See Docker container persistence
floydhub/dl-docker:cpu 	This the image that you want to run. The format is image:tag. In our case, we use the image dl-docker and tag gpu or cpu to spin up the appropriate image
bash 	This provides the default command when the container is started. Even if this was not provided, bash is the default command and just starts a Bash session. You can modify this to be whatever you'd like to be executed when your container starts. For example, you can execute docker run -it -p 8888:8888 -p 6006:6006 floydhub/dl-docker:cpu jupyter notebook. This will execute the command jupyter notebook and starts your Jupyter Notebook for you when the container starts




----------------------------------------
docker pull rgomes/jessie-torch7
----------------------------------------

Description

More info: https://github.com/frgomes/Dockerfiles/tree/legacy/legacy/jessie-torch7

----------------------------------------
docker pull argusyk/opencv_torch_cuda
----------------------------------------
OpenCV 2.4.13 + Torch7 + CUDA7.5 + Jupyter

----------------------------------------
docker pull argusyk/opencv_dlib_torch_cuda
----------------------------------------
OpenCV 2.4.13 + Torch7 + Dlib 19.2 + CUDA7.5 + CuDNN 5.x + Jupyter +Many Others (Pillow, matplotlib, scipy, scikit-learn)


----------------------------------------
docker pull dpit/neural-style-cpu
----------------------------------------
Torch implementation of neural style algorithm.

To run it:
docker run --rm -it dpit/neural-style-cpu

To test that it works:
cd ~/neural-style
th neural_style.lua -gpu -1 -print_iter 1
You should see the output as described at https://github.com/jcjohnson/neural-style/blob/master/INSTALL.md.

For further details see https://github.com/jcjohnson/neural-style.


----------------------------------------
docker pull dkrot/neural-cuda
----------------------------------------
Run Torch7 and CUDA
Preinstalled software to run Torch7 and CUDA. It's ready to launch https://github.com/jcjohnson/neural-style
(Use it with nvidia-docker)

----------------------------------------
docker pull rosetteapi/docker-rapidminer-gradle
----------------------------------------
Basic Gradle builder for RapidMiner extensions
Currently built using Gradle 2.6, which is what RapidMiner is using right now. To run:

docker run --rm -v <path-to-extension-root>:/source rosetteapi/docker-rapidminer-gradle

This will put the generated extension jar in an "extensions" directory under the <path-to-extension-root>



----------------------------------------
docker pull rapidminer/docker-registry-web
----------------------------------------
Docker pull command

docker pull hyper/docker-registry-web

How to run
Quick start (config with environment variables, no authentication)

Do not use registry as registry container name, it will break REGISTRY_NAME environment variable.

docker run -d -p 5000:5000 --name registry-srv registry:2
docker run -it -p 8080:8080 --name registry-web --link registry-srv -e REGISTRY_URL=http://registry-srv:5000/v2 -e REGISTRY_NAME=localhost:5000 hyper/docker-registry-web

Connecting to docker registry with basic authentication and self-signed certificate

docker run -it -p 8080:8080 --name registry-web --link registry-srv \
           -e REGISTRY_URL=https://registry-srv:5000/v2 \
           -e REGISTRY_TRUST_ANY_SSL=true \
           -e REGISTRY_BASIC_AUTH="YWRtaW46Y2hhbmdlbWU=" \
           -e REGISTRY_NAME=localhost:5000 hyper/docker-registry-web

No authentication, with config file

    Create configuration file config.yml

    (Any property in this config may be overridden with environment variable, for example
    property registry.auth.enabled will become REGISTRY_AUTH_ENABLED)

    registry:
      # Docker registry url
      url: http://registry-srv:5000/v2
      # Docker registry fqdn
      name: localhost:5000
      # To allow image delete, should be false
      readonly: false
      auth:
        # Disable authentication
        enabled: false

    Run with docker

    docker run -p 5000:5000 --name registry-srv -d registry:2
    docker run -it -p 8080:8080 --name registry-web --link registry-srv -v $(pwd)/config.yml:/conf/config.yml:ro hyper/docker-registry-web

    Web UI will be available on http://localhost:8080

With authentication enabled

----------------------------------------
docker pull google/nodejs-hello
----------------------------------------

google/nodejs-hello

google/nodejs-hello is a docker image for the express hello world application.

It is based on google/nodejs-runtime base image and listen on port 8080.
Usage

    Run the following command

      docker run -p 8080 google/nodejs-hello


----------------------------------------
docker pull google/golang-hello
----------------------------------------


Usage

Run the following command:

    docker run -p 8080 google/golang-hello

Run with a custom port:

    docker run -e PORT=9292 -p 9292 google/golang-hello

----------------------------------------
docker pull google/python-hello
----------------------------------------

Usage

    Run the following command

      docker run -p 8080 google/python-hello



----------------------------------------
docker pull aptalca/docker-amazon-echo-ha-bridge
----------------------------------------

Install On Other Platforms (like Ubuntu or Synology 5.2 DSM, etc.):

On other platforms, you can run this docker with the following command:

docker run -d --name="AmazonEcho-HA-Bridge" --net="host" -e SERVERIP="192.168.X.X" -e SERVERPORT="XXXX" -v /path/to/config/:/config:rw -v /etc/localtime:/etc/localtime:ro aptalca/docker-amazon-echo-ha-bridge



----------------------------------------
docker pull guildeducation/docker-amazon-redshift
----------------------------------------

How to use the image
Running

docker run -d --name my-redshift guildeducation/docker-amazon-redshift

Port 5439 is set via an EXPOSE command so it should be available to linked containers. Optionally, you can map it to the host:

docker run -d -p 5439:5439 --name my-redshift guildeducation/docker-amazon-redshift
Persisting Data

In order to have data persist between container runs, map the PGDATA directory to a volume. This variable is defaulted to /var/lib/postgresql/data:

docker run -d -p 5439:5439 -v /path/on/host:/var/lib/postgresql/data --name my-redshift guildeducation/docker-amazon-redshift

initdb is run on the PGDATA directory automatically on container start
Setting a password

It is recommended to set a password for the default postgres user:

docker run -d -p 5439:5439 -v /path/on/host:/var/lib/postgresql/data -e POSTGRES_PASSWORD=your_password --name my-redshift guildeducation/docker-amazon-redshift
Environment Variables

    PGPORT - port Postgres will run on. Default is 5439.
    PGDATA - Data directory for Postgres. Default is /var/lib/postgresql/data.
    POSTGRES_PASSWORD - password for the database user. Default is no password.
    POSTGRES_USER - Sets a different user to run the database under. Default is postgres.
    POSTGRES_DB - Sets the name of the db that is created during the initdb run. Note the default postgres db will be created anyway.
    POSTGRES_INITDB_ARGS - Sets additional initdb args.
    POSTGRES_INITDB_XLOGDIR - Defines a different location for the Postgres transaction log

----------------------------------------
docker pull masterandrey/docker-amazon-dash-button-hack
----------------------------------------


I use it on my Synology.

To run it:

docker rm -f amazon_dash
docker run --net host -it --name amazon_dash -v $PWD/amazon-dash-private:/amazon-dash-private:ro masterandrey/docker-amazon-dash-button-hack

In folder amazone-dash-private you should have:

    settings settings.json
    buttons list buttons.json
    amazon-dash-hack.json with google API credentials Google Sheets, Google Calendar
    ifttt-key.json with Maker Webhook key



----------------------------------------
docker pull jarvice/ubuntu-ibm-mldl-ppc64le
----------------------------------------

Ubuntu + IBM-optimized ML/DL for POWER8 base image



----------------------------------------
docker pull ibmcom/ibm-http-server
----------------------------------------
Official IBM HTTP Server for WebSphere Application Server image

Usage

This Docker image contains IBM HTTP Server, IBM WebServer Plugins and IBM WebSphere Customization Tools. The image can be started as follows:

    Starting IBM HTTP Server container in daemon mode:

    docker run --name ihs -h ihs -p 80:80 –d ibmcom/ibm-http-server

    Starting IBM HTTP Server container in interactive mode:

    docker run --name ihs -h ihs -p 80:80 –it ibmcom/ibm-http-server bash
      /opt/IBM/HTTPServer/bin/apachectl start


----------------------------------------
docker pull linarotechnologies/ibm-bluemix-mosquitto
----------------------------------------

Mosquitto for IBM Bluemix
Build the container

docker build -t ibm-bluemix-mosquitto --force-rm -f Dockerfile .

Run the container

Create a local environment file containing the IBM Bluemix authentication keys:

$ cat ibm-bluemix-mosquitto.env
BLUEMIX_AUTHKEY=a-org-key
BLUEMIX_AUTHTOKEN=token
BLUEMIX_ORG=bluemixorg
GW_DEVICE_TYPE=hikey

Then run the containiner giving your local environment file with --env-file:

docker run --restart=always -d -t --net=host --env-file=/home/linaro/ibm-bluemix-mosquitto.env --name ibm-bluemix-mosquitto ibm-bluemix-mosquitto

Run the pre-built container

docker run --restart=always -d -t --net=host --env-file=/home/linaro/ibm-bluemix-mosquitto.env --name ibm-bluemix-mosquitto linarotechnologies/ibm-bluemix-mosquitto:latest

----------------------------------------
docker pull ibmcom/datapower
----------------------------------------

License Acceptance

To use the image, you must accept the license terms. If you do not assert that you have accepted the license, DataPower will prevent most tasks.

You can view the license with the /bin/drouter --show-license arg or by specifying DATAPOWER_SHOW_LICENSE=true

docker run -e DATAPOWER_SHOW_LICENSE=true ibmcom/datapower
docker run ibmcom/datapower /bin/drouter --show-license

You can assert license acceptance by specifying the environment variable DATAPOWER_ACCEPT_LICENSE=true or with the /bin/drouter --accept-license arg when you run the image.

docker run -e DATAPOWER_ACCEPT_LICENSE=true ibmcom/datapower
docker run ibmcom/datapower /bin/drouter --accept-license

An example of using DataPower configuration in Docker Volumes:

docker run -v /path/to/my/config:/drouter/config -v /path/to/my/local:/drouter/local ibmcom/datapower

If you don't already have configuration, you can enable web-mgmt, map port 9090, and import or create a new configuration. When you write mem or Save Configuration, the resulting files will be written to the volumes.

utting it all together

One common way a Developer might run DataPower is:

docker run -it \
  -v $PWD/config:/drouter/config \
  -v $PWD/local:/drouter/local \
  -e DATAPOWER_ACCEPT_LICENSE=true \
  -e DATAPOWER_INTERACTIVE=true \
  -e DATAPOWER_WORKER_THREADS=4 \
  -p 9090:9090 \
  ibmcom/datapower

The Docker volumes allow the developer to read and save configuration from and to files that are under version control.


----------------------------------------
docker pull ibmcom/iop-hadoop
----------------------------------------
IBM Open Platform with Apache Hadoop

Run docker image directly:

docker run -d --name iop-hadoop --privileged=true \
--net=host -p 8080:8080 -p 8670:8670 \
-p 8440:8440 -p 8441:8441 -p 50010:50010 \
-p 50020:50020 -p 50070:50070 -p 8188:8188 \
-p 8190:8190 -p 10200:10200 -p 8020:8020 \
-p 50075:50075 -p 60010:60010 -p 60020:60020 \
-p 10000:10000 -p 8088:8088 -p 50060:50060 \
-p 8032:8032 -p 2022:22 -p 80:80 \
--ulimit nproc=65535 --ulimit nofile=65535 \
--ulimit core=65535 ibmcom/iop-hadoop
docker exec -it iop-hadoop ambari-server start

----------------------------------------
docker pull krsyoung/ibm-containers-ci
----------------------------------------

Development

Build the image:

docker build -t ibm-containers-ci:latest .

Run the image:

docker run -it --rm ibm-containers-ci:latest


----------------------------------------
docker pull medgateag/ibm-base-image
----------------------------------------

Usage

docker run -it medgateag/ibm-base-image:{tag}



----------------------------------------
docker pull dennisseidel/ibm-integration-bus-docker
----------------------------------------

    Dockerfile. see Docker Documentation.

# build the runtime image
docker build -t iib-app-image .
# start the runtime image
docker run -p 7800:7800 -p 7843:7843 -p 4414:4414 iib-app-image


----------------------------------------
docker pull ibmcom/iotvisualization
----------------------------------------



Run the container

docker run -it --privileged -p 9088:9088 -p 8090:8090 -p 50000:50000 ibmcom/iotvisualization

    -p 50000:50000 exposes port 50000 to allow connections from the remote client.



----------------------------------------
docker pull biginsights/iop-hadoop
----------------------------------------


IBM Open Platform Development Release

This image contains latest and greatest alpha quality test image for development. For official GA release, please check out ibmcom/iop-hadoop.

To run this image with docker:

docker pull biginsights/iop-hadoop
docker run -d --name iop-hadoop --privileged=true \
--net=host -p 8080:8080 -p 8670:8670 \
-p 8440:8440 -p 8441:8441 -p 50010:50010 \
-p 50020:50020 -p 50070:50070 -p 8188:8188 \
-p 8190:8190 -p 10200:10200 -p 8020:8020 \
-p 50075:50075 -p 60010:60010 -p 60020:60020 \
-p 10000:10000 -p 8088:8088 -p 50060:50060 \
-p 8032:8032 -p 2022:22 -p 80:80 \
--ulimit nproc=65535 --ulimit nofile=65535 \
iop-hadoop
docker exec -it iop-hadoop /etc/start-all.sh


----------------------------------------
docker pull reachlin/bluemix
----------------------------------------

Basic Usage:

docker run --name bluemix -d --privileged reachlin/bluemix
docker exec bluemix cf login -u "your email" -p "your password" -o "orgnization" -s "space" -a api.ng.bluemix.net
docker exec bluemix cf ic init
docker exec bluemix cf ic images

k8s Usage:

docker run --name bluemix -d --privileged -p 8088:8081 reachlin/bluemix
docker exec -it bluemix bash

Advanced Usage:

Use docker on your host to push and pull images. This image includes docker client, by mounting the container on the host's docker socket,
you can run docker in the container. Please google DIND for more details.

docker run --name bluemix -d --privileged -v /var/run/docker.sock:/var/run/docker.sock reachlin/bluemix
docker exec bluemix docker images

Last updated 04/21/2017, upgrade os, docker, kubectl



----------------------------------------
docker pull oems/apache-php-ibm_db2
----------------------------------------

IBM DB2, Cloudscape and Apache Derby support with Module release 1.9.7 for PHP
FROM php:5.5.32-apache with ibm_db2 connector for DB2 database.


Without a Dockerfile

If you don't want to include a Dockerfile in your project, it is sufficient to do the following:

$ docker run -d -p 80:80 --name my-apache-php-db2 -v "$PWD":/var/www/html oems/apache-php-ibm_db2

----------------------------------------
docker pull davidkassa/yarn-angular-cli-firebase-tools
----------------------------------------

Can be pulled from Docker Hub

docker pull davidkassa/yarn-angular-cli-firebase-tools


----------------------------------------
docker pull elboletaire/angular-filemanager
----------------------------------------

Usage

You can create a container both using docker run or docker-compose.

Using docker run:

docker run -d -p 8080:80 \
    -e BRIDGE=php-local \
    -v $PWD:/var/www/files \
    elboletaire/angular-filemanager

Using docker-compose (take a look to the one provided in this repo):

docker-compose up -d


BRIDGE:

docker run -d -p 8080:80 \
    -e BRIDGE=my-bridge-implementation \
    -v $PWD:/var/www/files \
    elboletaire/angular-filemanager

This would require a configuration file named
configuration-my-bridge-implementation.js.



----------------------------------------
docker pull luishmcmoreno/angular-cli-docker
----------------------------------------


Angular 2 App. Simply call

docker run -it --rm --name create-angular-project -v "$PWD":/home/app angular-cli:1.0.0-beta.24 ng new -sn APP_NAME

Obviously you should replace APP_NAME with the name of the app you like to build.

docker-compose build

Serving your new App

Finally you can call docker-compose up to serve your new app. It will be accessible on http://localhost:4200/. You can see that everything works as expected, if a website with "app works" is shown in your browser.



----------------------------------------
docker pull bencuk/angular-demoapp
----------------------------------------


Azure storage account and it's access key

docker run -d -p 3000 -e APPSETTING_STORAGE_ACCOUNT=myacct -e APPSETTING_STORAGE_KEY=123supersecret123 bencuk/angular-demoapp:latest

Running in dev mode with in memory database is easier

docker run -d -p 3000 bencuk/angular-demoapp:dev

----------------------------------------
docker pull moviemasher/angular-moviemasher
----------------------------------------


Docker Usage

Docker's moviemasher/angular-moviemasher image is automatically built from the official php:apache image, adding some configuration as well as copying this project into web root. To make the UI available at your docker IP on port 8080:

    docker run -d -p 8080:80 --name=angular_moviemasher moviemasher/angular-moviemasher

All functions should be available at this juncture, except uploading and rendering which will just trigger the saving of a job description file into queue_directory. Because there is a VOLUME instruction for this directory, it can be mounted by other containers - we'll attach it to one run from the moviemasher/moviemasher.rb image which will handle the actual transcoding operation:

    docker run -d -t --volumes-from=angular_moviemasher --name=moviemasher_rb moviemasher/moviemasher.rb process_loop

To stop and remove the containers:

    docker stop angular_moviemasher
    docker rm angular_moviemasher
    docker stop moviemasher_rb
    docker rm moviemasher_rb

The project also includes several docker-compose files in version 2 format, so you might need to update your Docker installation in order to utilize them. The simplest does what the commands above do - after downloading the repository cd into the config/docker/production directory and execute:

    docker-compose up -d

To stop and remove the containers:

    docker-compose down -v


Or if docker is being used cd into the config/docker/grunt or config/docker/composer directory and execute...

    docker-compose run --rm grunt

    docker-compose run --rm grunt bower install --production

    docker-compose run --rm composer

docker-compose restart moviemasher_rb

To stop and remove the containers:

docker-compose down -v




----------------------------------------
docker pull schuermann/s2i-angular-container
----------------------------------------


S2I - Source 2 Image

You can directly invoke S2I builds from command line using the S2I binary.

This repository contains an angular app (ng new test-app) in the directory test/test-app that can be used for demo purpose. To build this demo app with S2i simply execute:

s2i build https://github.com/MrGoro/s2i-angular-container.git --context-dir=test/test-app/ schuermann/s2i-angular-container angular-sample-app
docker run -p 8080:8080 angular-sample-app

Incremental Builds

You can trigger incremental builds by specifying --incremental=true when building an image. Incremental builds provide the already installed node_modules directory from a previous build within a following build. This will dramatically speed up installation of NodeJS dependencies.

s2i build https://github.com/MrGoro/s2i-angular-container.git --incremental=true --context-dir=test/test-app/ schuermann/s2i-angular-container angular-sample-app
docker run -p 8080:8080 angular-sample-app

To use a different image for runtime, you can do the following with S2I:

s2i build https://github.com/MrGoro/s2i-angular-container.git --context-dir=test/test-app/ schuermann/s2i-angular-container angular-sample-app --runtime-image <runtime-image> --runtime-artifact </path/to/artifact>

For example to run the built app using nginx you could use the following:

s2i build https://github.com/MrGoro/s2i-angular-container.git --context-dir=test/test-app/ schuermann/s2i-angular-container angular-sample-app --runtime-image nginx --runtime-artifact /opt/app-root/src:/usr/share/nginx/html


----------------------------------------
docker pull bradojevic/angular-sendbox
----------------------------------------
https://hub.docker.com/r/bradojevic/angular-sendbox/

docker stop angular-sandbox; docker rm angular-sandbox; docker create --name angular-sandbox -p 9000:9000 -p 35729:35729 -v $(pwd)/src:/home/yeoman/src bradojevic/angular-sandbox && docker start angular-sandbox

In order for serve to work we need to change grunt config so you should have
grunt.initConfig updated to:
connect: {
main: {
options: {
port: 9000,
hostname: '0.0.0.0',
livereload: 35729
}
}
},

docker exec -it angular-sandbox grunt serve



----------------------------------------
docker pull mdunhem/docker-angular-jenkins
----------------------------------------

How to build the image

First build it:
docker build --rm --no-cache --pull --tag "<YOUR-USERNAME>/docker-angular-jenkins" .

Then publish it:
docker push <YOUR-USERNAME>/docker-angular-jenkins

Contains

    Node - 8
    npm - 5.3.0
    Angular CLI - 1.2.3
    Chrome (run in no-sandbox mode) - latest stable

Available Build Arguments

    ANGULAR_CLI_VERSION - default: 1.2.3
    GID - default: 988
    UID - default: 406

